<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Stevan A" />
  <title>Stevan's notes – Elastically scalable thread pools</title>
  <link rel="stylesheet" href="style.css" />
  <link rel="alternate" type="application/rss+xml"
        title="RSS feed"
        href="rss.xml" />
  <script data-goatcounter="https://stevana-github-io.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
  <nav id="nav">
    <span class="title"><a href="index.html">Stevan's notes...</a></span>
    <a href="about.html">About</a>
    <a href="rss.xml">Feed <img height="10px" src="rss.svg" /></a>
  </nav>
</header>
<hr />
<main>
<h1>Elastically scalable thread pools</h1>
<nav id="TOC" class="sidenote" role="doc-toc">
<h2 id="toc-title">Table of contents</h2>
<ul>
<li><a href="#motivation" id="toc-motivation">Motivation</a></li>
<li><a href="#plan" id="toc-plan">Plan</a></li>
<li><a href="#pseudo-code" id="toc-pseudo-code">Pseudo-code</a>
<ul>
<li><a href="#main" id="toc-main">Main</a></li>
<li><a href="#worker-pool" id="toc-worker-pool">Worker pool</a></li>
<li><a href="#load-generator" id="toc-load-generator">Load
generator</a></li>
<li><a href="#pid-controller" id="toc-pid-controller">PID
controller</a></li>
</ul></li>
<li><a href="#how-it-works" id="toc-how-it-works">How it works</a></li>
<li><a href="#usage" id="toc-usage">Usage</a></li>
<li><a href="#contributing" id="toc-contributing">Contributing</a></li>
<li><a href="#see-also" id="toc-see-also">See also</a></li>
<li><a href="#discussion" id="toc-discussion">Discussion</a></li>
</ul>
</nav>
<p>An experiment in controlling the size of a thread pool using a PID
controller.</p>
<h2 id="motivation">Motivation</h2>
<p>A tried and tested way to achieve parallelism is to use pipelining.
It’s used extensively in manufacturing and in computer hardware.</p>
<p>For example, Airbus <a
href="https://youtu.be/oxjT7veKi9c?t=2682">apparently</a> outputs two
airplanes per day on average, even though it takes two months to build a
single airplane from start to finish. It’s also used inside CPUs to <a
href="https://en.wikipedia.org/wiki/Instruction_pipelining">pipeline
instructions</a>.</p>
<p>Let’s imagine we want to take advantage of pipelining in some
software system. To make things more concrete, let’s say we have a
system where some kind of requests come on over the network and we want
to process them in some way. The first stage of the pipeline is to parse
the incoming requests from raw bytestrings into some more structured
data, the second stage is to apply some validation logic to the parsed
data and the third stage is to process the valid data and produce some
outputs that are then sent back to the client or stored somewhere.</p>
<p><img
src="https://raw.githubusercontent.com/stevana/elastically-scalable-thread-pools/main/img/pipeline.svg" /></p>
<p>The service time of an item can differ from stage to stage, for
example parsing might be slower than validation, which can create
bottlenecks. Luckily it’s quite easy to spot bottlenecks by merely
observing the queue lengths and once a slow stage is found we can often
fix it by merely adding an additional parallel processor to that stage.
For example we could spin up two or more threads that take bytestrings
from the first queue and turn them into structured data and thereby
compensate for parsing being slow.</p>
<p>By spinning up more threads we can decrease latency (waiting time in
the queue) and increase throughput (process more items), but we are also
on the other hand using more energy and potentially hogging CPU
resources that might be better used elsewhere in the pipeline or system
at large.</p>
<p>So here’s the question that the rest of this post is concerned about:
can we dynamically spin up and spin down threads at a stage in response
to the input queue length for that stage?</p>
<h2 id="plan">Plan</h2>
<p>Let’s focus on a single stage of the pipeline to make things easier
for ourselves.</p>
<p><img
src="https://raw.githubusercontent.com/stevana/elastically-scalable-thread-pools/main/img/stage.svg" /></p>
<p>We’d like to increase the parallelism of the processors if the input
queue grows, and decrease it when the queue shrinks. One simple strategy
might be to establish thresholds, i.e. if there’s over <span
class="math inline">100</span> items in the input queue then allocate
more processors and if there’s no items in the queue then deallocate
them.</p>
<p>Since allocating and deallocating processors can be an expense in
itself, we’d like to avoid changing them processor count
unnecessarily.</p>
<p>The threshold based approach is sensitive to unnecessarily changing
the count if the arrival rate of work fluctuates. The reason for this is
because it only takes the <em>present</em> queue length into
account.</p>
<p>We can do better by also incorporating the <em>past</em> and trying
to predict the <em>future</em>, this is the basic idea of <a
href="https://en.wikipedia.org/wiki/PID_controller">PID controllers</a>
from <a href="https://en.wikipedia.org/wiki/Control_theory">control
theory</a>.</p>
<p>Here’s what the picture looks like with a PID controller in the
loop:</p>
<pre><code>                                            +----------------------------------+
                                            |                                  |
    -------------------------------------------&gt;[Input queue]--&gt;[Worker pool]-----&gt;[Output queue]--&gt;
                                            |                                  |
     r(t)   e(t)                    u(t)    |                                  |
    -----&gt;+------&gt;[PID controller]--------&gt; |                                  |
          ^                                 |                                  |
          |                                 +----------------------------------+
          |                                                 | y(t)
          +-------------------------------------------------+
</code></pre>
<p>The PID controller monitors the queue length <span
class="math inline"><em>y</em>(<em>t</em>)</span>, compares it to some
desired queue length <span
class="math inline"><em>r</em>(<em>t</em>)</span> (also known as the
setpoint) and calculates the error <span
class="math inline"><em>e</em>(<em>t</em>)</span>. The error determines
the control variable <span
class="math inline"><em>u</em>(<em>t</em>)</span> which is used to grow
or shrink the processor pool.</p>
<h2 id="pseudo-code">Pseudo-code</h2>
<p>Let’s start top-down with the <code>main</code> function which drives
our whole experiment.</p>
<h3 id="main">Main</h3>
<pre><code>main =

  // Create the in- and out-queues.
  inQueue  := newQueue()
  outQueue := newQueue()


  // The workers don&#39;t do anything interesting, they merely sleep for a bit to
  // pretend to be doing some work.
  worker := sleep 0.025s

  // Create an empty worker pool.
  pool := newPool(worker, inQueue, outQueue)

  // Start the PID controller in a background thread. The parameters provided
  // here allow us to tune the PID controller, we&#39;ll come back to them later.
  kp := 1
  ki := 0.05
  kd := 0.05
  dt := 0.01s
  fork(pidController(kp, ki, kd, dt, pool))


  // Create a workload for our workers. We use the sine function to create
  // between 0 and 40 work items every 0.1s for 60s. The idea being that because
  // the workload varies over time the PID controller will have some work to do
  // figuring out how many workers are needed.
  sineLoadGenerator(inQueue, 40, 0.1s, 60s)</code></pre>
<h3 id="worker-pool">Worker pool</h3>
<p>The worker pool itself is merely a struct which packs up the
necessary data we need to be able to scale it up and down.</p>
<pre><code>struct Pool =
  { inQueue:  Queue&lt;Input&gt;
  , outQueue: Queue&lt;Output&gt;
  , worker:   Function&lt;Input, Output&gt;
  , pids:     List&lt;ProcessId&gt;
  }</code></pre>
<p>Creating a <code>newPool</code> creates the struct with an empty list
of process ids.</p>
<pre><code>newPool worker inQueue outQueue = Pool { ..., pids: emptyList }</code></pre>
<p>Scaling up and down are functions that take and return a
<code>Pool</code>.</p>
<pre><code>scaleUp pool =
  work := forever
            x := readQueue(pool.inQueue)
            y := pool.worker(x)
            writeQueue(pool.outQueue, y)
  pid   := fork(work)
  pool&#39; := pool.pids = append(pid, pool.pids)
  return pool&#39;</code></pre>
<p>The function <code>scaleDown</code> does the inverse, i.e. kills and
removes the last process id from <code>pool.pids</code>.</p>
<h3 id="load-generator">Load generator</h3>
<p>In order to create work load that varies over time we use the sine
function. The sine function oscillates between <span
class="math inline"> − 1</span> and <span
class="math inline">1</span>:</p>
<p><img
src="https://raw.githubusercontent.com/stevana/elastically-scalable-thread-pools/main/img/sine.svg" /></p>
<p>We would like to have it oscillate between <span
class="math inline">0</span> and some max value <span
class="math inline"><em>m</em></span>. By multiplying the output of the
sine function by <span class="math inline"><em>m</em>/2</span> we get an
oscillation between <span class="math inline"> − <em>m</em>/2</span> and
<span class="math inline"><em>m</em>/2</span>, we can then add <span
class="math inline"><em>m</em>/2</span> to make it oscillate between
<span class="math inline">0</span> and <span
class="math inline"><em>m</em></span>.</p>
<p>We’ll sample the resulting function once every <code>timesStep</code>
seconds, this gives us the amount of work items (<code>n</code>) to
create we then spread those out evenly in time, rinse and repeat until
we reach some <code>endTime</code>.</p>
<pre><code>sineLoadGenerator inQueue workItem maxItems timeStep endTime =
  for t := 0; t &lt; endtime; t += timeStep
    n := sin(t) * maxItems / 2 + maxItems / 2
    for i := 0; i &lt; n; i++
      writeQueue(inQueue, workItem)
      sleep(timeStep / n)</code></pre>
<h3 id="pid-controller">PID controller</h3>
<p>The PID controller implementation follows the pseudo-code given at <a
href="https://en.wikipedia.org/wiki/PID_controller#Pseudocode">Wikipedia</a>:</p>
<pre><code>previous_error := 0
integral := 0
loop:
   error := setpoint − measured_value
   proportional := error;
   integral := integral + error × dt
   derivative := (error − previous_error) / dt
   output := Kp × proportional + Ki × integral + Kd × derivative
   previous_error := error
   wait(dt)
   goto loop</code></pre>
<p>Where <code>Kp</code>, <code>Ki</code> and <code>Kd</code> is
respectively the proportional, integral and derivative gain and
<code>dt</code> is the loop interval time. The proportional part acts on
the <em>present</em> error value, the integral acts on the <em>past</em>
and the derivative tries to predict the <em>future</em>. The measured
value is the input queue length and the setpoint, i.e. desired queue
length, is set to zero. If the <code>output</code> of the PID controller
is less than <span class="math inline"> − 100</span> (i.e. the queue
length is over <span class="math inline">100</span> taking the present,
past and possible future into account) then we scale up and if it’s more
than <span class="math inline"> − 20</span> (i.e. the queue length is
less than <span class="math inline">20</span>) then we scale down the
worker pool.</p>
<h2 id="how-it-works">How it works</h2>
<p>We start off by only setting the proportional part and keeping the
integral and derivative part zero, this is called a P-controller. We see
below that it will scale the worker count up and down proportionally to
the sine wave shaped load:</p>
<p><img
src="https://raw.githubusercontent.com/stevana/elastically-scalable-thread-pools/main/img/elastically-scalable-thread-pools-1.0-0.0-0.0.svg" /></p>
<p>A P-controller only focuses on the <em>present</em>, and we see that
it allocates and deallocates workers unnecessarily. In order to smooth
things out we introduce the integral part, i.e. a PI-controller. The
integral part takes the <em>past</em> into account. We see now that the
worker count stabilises at <span class="math inline">28</span>:</p>
<p><img
src="https://raw.githubusercontent.com/stevana/elastically-scalable-thread-pools/main/img/elastically-scalable-thread-pools-1.0-5.0e-2-0.0.svg" /></p>
<p>We can improve on this by adding the derivative part which takes the
<em>future</em> into account. We then see that it stabilises at <span
class="math inline">26</span> workers:</p>
<p><img
src="https://raw.githubusercontent.com/stevana/elastically-scalable-thread-pools/main/img/elastically-scalable-thread-pools-1.0-5.0e-2-5.0e-2.svg" /></p>
<p>With the full PID controller, which stabilises using less workers
than the PI-controller, we see that the queue length spikes up to <span
class="math inline">20</span> or so each time the work load generator
hits one of the sine function’s peaks. Recall that we started scaling
down once the queue length was less than <span
class="math inline">20</span>.</p>
<h2 id="usage">Usage</h2>
<p>The above graphs were generated by running:
<code>cabal run app -- kp ki kd</code>, where the <span
class="math inline"><em>K</em><sub><em>p</em></sub></span>, <span
class="math inline"><em>K</em><sub><em>i</em></sub></span>, and <span
class="math inline"><em>K</em><sub><em>d</em></sub></span> parameters
are the tuning parameters for the PID controller.</p>
<p>If you don’t have the GHC Haskell compiler and the <code>cabal</code>
build tool already installed, then the easiest way to get it is via <a
href="https://www.haskell.org/ghcup/"><code>ghcup</code></a>.
Alternatively if you got <code>nix</code> then <code>nix-shell</code>
should give give you access to all the dependencies you need.</p>
<h2 id="contributing">Contributing</h2>
<p>There are many ways we can build upon this experiment, here are a few
ideas:</p>
<ul class="task-list">
<li><input type="checkbox" />We probably want to limit the max number of
threads in a pool;</li>
<li><input type="checkbox" /><a
href="https://github.com/m-lundberg/simple-pid/blob/master/simple_pid/pid.py#L128">Clamp</a>
integral part to avoid integral windup;</li>
<li><input type="checkbox" />If two or more threads take items from some
input queue and put them on some output queue then there’s no guarantee
that the order of the output items will be the same as the input items.
We could solve this, and regain determinism, by using array based queues
and shard on the index, i.e. even indices goes to one processor and odd
to an other or more generally modulus N can be used to shard between N
processors. This is essentially what the <a
href="https://en.wikipedia.org/wiki/Disruptor_(software)">LMAX
Disruptor</a> does;</li>
<li><input type="checkbox" />We’ve only looked at one stage in a
pipeline, what happens if we have multiple stages? is it enough to
control each individual stage separately or do we need more global
control?</li>
<li><input type="checkbox" />Can we come up with other things to
control? E.g. batch sizes?</li>
<li><input type="checkbox" />We’ve only monitored the current queue
length, could we combine this with other data? E.g. time series of the
queue length from the previous day?</li>
<li><input type="checkbox" />Is it robust to wildly changing usage
patterns? E.g. bursty traffic or the <a
href="https://en.wikipedia.org/wiki/Slashdot_effect">Slashdot
effect</a>?</li>
<li><input type="checkbox" />We’ve looked at scaling up and down on a
single machine (vertical scaling), what about scaling out and in across
multiple machines (horizontal scaling)?</li>
<li><input type="checkbox" />We generated and processed real work items
(by sleeping), could we do a discrete-event simulation instead to avoid
having to wait for the sleeps?</li>
<li><input type="checkbox" />I just picked random values for the PID
controller parameters, there are more principled <a
href="https://en.wikipedia.org/wiki/PID_controller#Overview_of_tuning_methods">ways</a>
of tuning the PID controller;</li>
<li><input type="checkbox" />The PID controller we implemented merely
followed the pseudo-code from Wikipedia, there’s probably better ways of
implementing it?</li>
</ul>
<p>If any of this sounds interesting, feel free to get in touch!</p>
<h2 id="see-also">See also</h2>
<ul>
<li><p><a
href="https://www.researchgate.net/publication/265611546_A_Review_of_Auto-scaling_Techniques_for_Elastic_Applications_in_Cloud_Environments"><em>A
Review of Auto-scaling Techniques for Elastic Applications in Cloud
Environments</em></a></p>
<ol start="2014" type="1">
<li>is a survey paper which talks about both threshold and PID
controllers;</li>
</ol></li>
<li><p><a
href="https://people.eecs.berkeley.edu/~brewer/papers/SEDA-sosp.pdf"><em>SEDA:
An Architecture for Well-Conditioned Scalable Internet Services</em></a>
(2001), this is paper that I got the idea for elastic scalable thread
pools. They use a threshold approach rather than a PID controller,
saying:</p>
<blockquote>
<p>The controller periodically samples the input queue (once per second
by default) and adds a thread when the queue length exceeds some
threshold (100 events by default). Threads are removed from a stage when
they are idle for a specified period of time (5 seconds by default).</p>
</blockquote>
<p>But also:</p>
<blockquote>
<p>Under SEDA, the body of work on control systems can be brought to
bear on service resource management, and we have only scratched the
surface of the potential for this technique.</p>
</blockquote>
<p>A bit more explanation is provided by Matt Welsh, who is one of the
author, in his PhD <a
href="https://cs.uwaterloo.ca/~brecht/servers/readings-new/mdw-phdthesis.pdf">thesis</a>
(2002):</p>
<blockquote>
<p>A benefit to ad hoc controller design is that it does not rely on
complex models and parameters that a system designer may be unable to
understand or to tune. A common complaint of classic PID controller
design is that it is often difficult to understand the effect of gain
settings.</p>
</blockquote></li>
<li><p>There are many introductory text books on control theory, but
there’s a lot less resources on how to apply control theory to software
systems. Here are a few resources:</p>
<ul>
<li><p><a
href="https://janert.org/books/feedback-control-for-computer-systems/"><em>Feedback
Control for Computer Systems</em></a> book by Philipp K. Janert
(2013);</p></li>
<li><p><a
href="https://www.cse.wustl.edu/~lu/control-tutorials/im09/"><em>Tutorial:
Recent Advances in the Application of Control Theory to Network and
Service Management</em></a>.</p></li>
</ul></li>
<li><p>It could very well be that the way we’ve applied classic PID
controllers isn’t suitable for unpredictable internet traffic loads.
There are branches of control theory might be better suited for this,
see, for example, <a
href="https://en.wikipedia.org/wiki/Robust_control">robust</a> and <a
href="https://en.wikipedia.org/wiki/Adaptive_control">adaptive</a>
control theory;</p></li>
<li><p>The .NET thread pool apparently uses the <a
href="https://en.wikipedia.org/wiki/Hill_climbing">hill climbing</a>
optimisation technique to <a
href="https://mattwarren.org/2017/04/13/The-CLR-Thread-Pool-Thread-Injection-Algorithm/">elastically
scale</a>;</p></li>
<li><p>My previous post: <a
href="https://github.com/stevana/pipelined-state-machines#pipelined-state-machines"><em>An
experiment in declaratively programming parallel pipelines of state
machines</em></a>.</p></li>
</ul>
<h2 id="discussion">Discussion</h2>
<ul>
<li><a href="https://news.ycombinator.com/item?id=35148068">Hacker
News</a>;</li>
<li><a
href="https://lobste.rs/s/ybtxic/experiment_elastically_scaling_thread">lobste.rs</a>;</li>
<li><a
href="https://old.reddit.com/r/haskell/comments/11qyfw7/an_experiment_in_elastically_scaling_a_thread/">r/haskell</a>;</li>
<li>Also see Glyn Normington’s <a
href="https://github.com/stevana/elastically-scalable-thread-pools/issues/1">comment</a>
in the issue tracker.</li>
</ul>
</main>
</body>
</html>
