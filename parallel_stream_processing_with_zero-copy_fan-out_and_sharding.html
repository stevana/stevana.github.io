<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Stevan Andjelkovic" />
  <title>Stevan's notes – Parallel stream processing with zero-copy fan-out and sharding</title>
  <link rel="stylesheet" href="style.css" />
  <link rel="alternate" type="application/rss+xml"
        title="RSS feed"
        href="rss.xml" />
  <script data-goatcounter="https://stevana-github-io.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
  <nav id="nav">
    <span class="title">Stevan's notes...</span>
    <a href="index.html">sorted by topic,</a>
    <a href="chronological.html">by time,</a>
    <a href="rss.xml">or as a feed <img height="10px" src="rss.svg" />!</a>
    <a href="about.html">What's this about?</a>
  </nav>
</header>
<hr />
<main>
<h1>Parallel stream processing with zero-copy fan-out and sharding</h1>
<nav id="TOC" class="sidenote" role="doc-toc">
<h2 id="toc-title">Table of contents</h2>
<ul>
<li><a href="#motivation-and-inspiration"
id="toc-motivation-and-inspiration">Motivation and inspiration</a></li>
<li><a href="#prior-work" id="toc-prior-work">Prior work</a></li>
<li><a href="#plan" id="toc-plan">Plan</a></li>
<li><a href="#list-transformer-model"
id="toc-list-transformer-model">List transformer model</a></li>
<li><a href="#queue-pipeline-deployment"
id="toc-queue-pipeline-deployment">Queue pipeline deployment</a></li>
<li><a href="#disruptor" id="toc-disruptor">Disruptor</a></li>
<li><a href="#disruptor-pipeline-deployment"
id="toc-disruptor-pipeline-deployment">Disruptor pipeline
deployment</a></li>
<li><a href="#observability"
id="toc-observability">Observability</a></li>
<li><a href="#running" id="toc-running">Running</a></li>
<li><a href="#further-work-and-contributing"
id="toc-further-work-and-contributing">Further work and
contributing</a></li>
<li><a href="#see-also" id="toc-see-also">See also</a></li>
</ul>
</nav>
<div class="date">Posted on Oct 11, 2023</div>
<p><em>Work in progress, please don’t share yet</em></p>
<p>In a previous <a
href="https://stevana.github.io/pipelined_state_machines.html">post</a>
I explored how we can make better use of our parallel hardware by means
of pipelining.</p>
<p>In a nutshell the idea of pipelining is to break up the problem in
stages and have one (or more) thread(s) per stage and then connect the
stages with queues. For example, imagine a service where we read some
request from a socket, parse it, validate, update our state and
construct a response, serialise the response and send it back over the
socket. These are six distinct stages and we could create a pipeline
with six CPUs/cores each working on a their own stage and feeding the
output to the queue of the next stage. If one stage is slow we can shard
the input, e.g. even requests to go to one worker and odd requests go to
another thereby nearly doubling the throughput for that stage.</p>
<p>One of the concluding remarks to the previous post is that we can
gain even more performance by using a better implementation of queues,
e.g. the <a
href="https://en.wikipedia.org/wiki/Disruptor_(software)">LMAX
Disruptor</a>.</p>
<p>The Disruptor is a low-latency high-throughput queue implementation
with support for multi-cast (many consumers can in parallel process the
same event), batching (both on producer and consumer side),
back-pressure, sharding (for scalability) and dependencies between
consumers.</p>
<p>In this post we’ll recall the problem of using “normal” queues,
discuss how Disruptor helps solve this problem and have a look at how we
can we provide a declarative high-level language for expressing
pipelines backed by Disruptors where all low-level details are hidden
away from the user of the library. We’ll also have a look at how we can
monitor and visualise such pipelines for debugging and performance
troubleshooting purposes.</p>
<h2 id="motivation-and-inspiration">Motivation and inspiration</h2>
<p>Before we dive into <em>how</em> we can achieve this, let’s start
with the question of <em>why</em> I’d like to do it.</p>
<p>I believe the way we write programs for multiprocessor networks,
i.e. multiple connected computers each with multiple CPUs/cores, can be
improved upon. Instead of focusing on the pitfalls of the current
mainstream approaches to these problems, let’s have a look at what to me
seems like the most promising way forward.</p>
<p>Jim Gray gave a great explanation of dataflow programming in this
Turing Award Recipient <a
href="https://www.youtube.com/watch?v=U3eo49nVxcA&amp;t=1949s">interview</a>.
He uses props to make his point, which makes it a bit difficult to
summaries in text here. I highly recommend watching the video clip, the
relevant part is only three minutes long.</p>
<p>The key point is exactly that of pipelining. Each stage is running on
a CPU/core, this program is completely sequential, but by connecting
several stages we create a parallel pipeline. Further parallelism (what
Jim calls partitioned parallelism) can be gained by partitioning the
inputs, by say odd and even sequence number, and feeding one half of the
inputs to one copy of the pipeline and the other half to another copy,
thereby almost doubling the throughput. Jim calls this a “natural” way
to achieve parallelism.</p>
<p>While I’m not sure if “natural” is the best word, I do agree that
it’s a nice way to make good use of CPUs/cores on a single computer
without introducing non-determinism. Pipelining is also effectively used
to achieve parallelism in manufacturing and hardware, perhaps that’s why
Jim calls it “natural”?</p>
<p>Things get a bit more tricky if we want to involve more computers.
Part of the reason, I believe, is that we run into the problem
highlighted by Barbara Liskov at the very end of her Turing award <a
href="https://youtu.be/qAKrMdUycb8?t=3058">lecture</a> (2009):</p>
<blockquote>
<p>“There’s a funny disconnect in how we write distributed programs. You
write your individual modules, but then when you want to connect them
together you’re out of the programming language and into this other
world. Maybe we need languages that are a little bit more complete now,
so that we can write the whole thing in the language.”</p>
</blockquote>
<p>Ideally we’d like our pipelines to seamlessly span over multiple
computers. In fact it should be possible to deploy same pipeline to
different configurations of processors without changing the pipeline
code (nor having to add any networking related code).</p>
<p>A pipeline that is redeployed with additional CPUs or computers might
or might not scale, it depends on whether it makes sense to partition
the input of a stage further or if perhaps the introduction of an
additional computer merely adds more overhead. How exactly the pipeline
is best spread over the available computers and CPUs/cores will require
some combination of domain knowledge, measurement and judgment.
Depending on how quick we can make redeploying of pipelines, it might be
possible to autoscale them using a program that monitors the queue
lengths.</p>
<p>Also related to redeploying, but even more important than
autoscaling, are upgrades of pipelines. That’s both upgrading the code
running at the individual stages, as well as how the stages are
connected to each other, i.e. the pipeline itself.</p>
<p>Martin Thompson has given many <a
href="https://www.youtube.com/watch?v=_KvFapRkR9I">talks</a> which echo
the general ideas of Jim and Barbara. If you prefer reading then you can
also have a look at the <a
href="https://www.reactivemanifesto.org/">reactive manifesto</a> which
he cowrote. Martin is also one of the people behind the Disruptor, which
we will come back to soon, and he also <a
href="https://youtu.be/OqsAGFExFgQ?t=2532">said</a> the following:</p>
<blockquote>
<p>“If there’s one thing I’d say to the Erlang folks, it’s you got the
stuff right from a high-level, but you need to invest in your messaging
infrastructure so it’s super fast, super efficient and obeys all the
right properties to let this stuff work really well.”</p>
</blockquote>
<p>This quote together with Joe Armstrong’s <a
href="https://youtu.be/bo5WL5IQAd0?t=2494">anecdote</a> of an unmodified
Erlang program <em>only</em> running 33 times faster on a 64 core
machine, rather than 64 times faster as per the Ericsson higher-up’s
expectations, inspired me to think about how one can improve upon the
already excellent work that Erlang is doing in this space.</p>
<p>Longer term, I like to think of pipelines spanning computers as a
building block for what Barbara <a
href="https://www.youtube.com/watch?v=8M0wTX6EOVI">calls</a> a
“substrate for distributed systems”. Unlike Barbara I don’t think this
substrate should be based on shared memory, but overall I agree with her
goal of making it easier to program distributed systems by providing
generic building blocks.</p>
<h2 id="prior-work">Prior work</h2>
<p>Working with streams of data is common. The reason for this is that
it’s a nice abstraction when dealing with data that cannot fit in
memory. The alternative is to manually load chunks of data one wants to
process into memory, load the next chunk etc, when we processes streams
this is hidden away from us.</p>
<p>Parallelism is a related problem, in that when one has big volumes of
data it’s also common to care about performance and how we can utilise
multiple processors.</p>
<p>Since dealing with limited memory and multiprocessors is a problem
that as bothered programmers and computer scientists for a long time, at
least since the 1960s, there’s a lot of work that has been done in this
area. I’m at best familiar with a small fraction of this work, so please
bear with me but also do let me know if I missed any important
development.</p>
<p>In 1963 Melvin Conway proposed <a
href="https://dl.acm.org/doi/10.1145/366663.366704">coroutines</a>,
which allows the user to conveniently process very large, or even
infinite, lists of items without first loading the list into memory,
i.e. streaming.</p>
<p>Shortly after, in 1965, Peter Landin introduced <a
href="https://dl.acm.org/doi/10.1145/363744.363749">streams</a> as a
functional analogue of Melvin’s imperative coroutines.</p>
<p>A more radical departure from Von Neumann style sequential
programming can be seen in the work on <a
href="https://en.wikipedia.org/wiki/Dataflow_programming">dataflow
programming</a> in general and especially in Paul Morrison’s <a
href="https://jpaulm.github.io/fbp/index.html">flow-based
programming</a> (late 1960s). Paul uses the following picture to
illustrate the similarity between flow-based programming and an assembly
line in manufacturing:</p>
<p><img
src="https://raw.githubusercontent.com/stevana/pipelining-with-disruptor/main/data/bottling_factory.png" /></p>
<p>Each stage is its own process running in parallel with the other
stages. In flow-based programming stages are computation and the
conveyor belts are queues. This gives us implicit parallelism and
determinate outcome.</p>
<p>Doug McIlroy, who was aware of some of the dataflow work<a
href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>, wrote a <a
href="http://doc.cat-v.org/unix/pipes/">memo</a> in 1964 about the idea
of pipes, although it took until 1973 for them to get implemented in
Unix by Ken Thompson. Unix pipes have a strong feel of flow-based
programming, although all data is of type string. A pipeline of commands
will start a process per command, so there’s implicit parallelism as
well (assuming the operative system schedules different processes on
different CPUs/cores). Fanning out can be done with <code>tee</code> and
process substitution,
e.g. <code>echo foo | tee &gt;(cat) &gt;(cat) | cat</code>, and more
complicated non-linear flows can be achieved with
<code>mkfifo</code>.</p>
<p>With the release of GNU <a
href="https://en.wikipedia.org/wiki/GNU_parallel"><code>parallel</code></a>
in 2010 more explicit control over parallelism was introduced as well as
the ability to run jobs on remote computers.</p>
<p>Around the same time many (functional) programming languages started
getting streaming libraries. Haskell’s <a
href="https://hackage.haskell.org/package/conduit">conduit</a> library
had its first release in 2011 and Haskell’s <a
href="https://hackage.haskell.org/package/pipes">pipes</a> library came
shortly after (2012). Java version 8, which has streams, was released in
2014. Both <a
href="https://clojure.org/reference/transducers">Clojure</a> and <a
href="https://doc.akka.io/docs/akka/current/stream/index.html">Scala</a>,
which also use the JVM, got streams that same year (2014).</p>
<p>Among the more imperative programming languages, JavaScript and
Python both have generators (a simple form of coroutines) since around
2006. Go has “goroutines”, a clear nod to coroutines, since its first
version (2009). Coroutines are also part of the C++20 standard.</p>
<p>Almost all of the above mentioned streaming libraries are intended to
be run on a single computer. Often they even run in a single thread,
i.e. not exploiting parallelism at all. Sometimes concurrent/async
constructs are available which create a pool of worker threads that
process the items concurrently, but they often break determinism
(i.e. rerunning the same computation will yield different results,
because the workers do not preserve the order of the inputs).</p>
<p>If the data volumes are too big for a single computer then there’s a
different set of streaming tools, such as Apache Hadoop (2006), Apache
Spark (2009), Apache Kafka (2011), Apache Storm (2011), and Apache Flink
(2011). While the Apache tools can often be deployed locally for testing
purposes, they are intended for distributed computations and are
therefore perhaps a bit more cumbersome to deploy and use than the
streaming libraries we mentioned earlier.</p>
<p>Initially it might not seem like a big deal that streaming libraries
don’t “scale up” or distributed over multiple computers, and that
streaming tools like the Apache ones don’t gracefully “scale down” to a
single computer. Just pick the right tool for the right job, right?
Well, it turns out that <a
href="https://youtu.be/XPlXNUXmcgE?t=2783">40-80%</a> of jobs submitted
to MapReduce systems (such as Apache Hadoop) would run faster if they
were ran on a single computer instead of a distributed cluster of
computers, so picking the right tool is perhaps not as easy as it first
seems.</p>
<p>There are two exceptions, that I know of, of streaming libraries that
also work in a distributed setting. Scala’s Akka/Pekko <a
href="https://doc.akka.io/docs/akka/current/stream/stream-refs.html">streams</a>
(2014) when combined with Akka/Pekko <a
href="https://github.com/apache/incubator-pekko-management">clusters</a>
and <a href="https://aeron.io/">Aeron</a> (2014). Aeron is the spiritual
successor of the Disruptor also written by Martin Thompson et al. The
Disruptor’s main use case was as part of the LMAX exchange. From what I
understand exchanges close in the evening (or at least did back then in
the case of LMAX), which allows for updates etc. These requirements
changed for Aeron where 24/7 operation was necessary and so distributed
stream processing is necessary where upgrades can happen without
processing stopping (or even slowing down).</p>
<p>Finally, I’d also like to mention functional reactive programming, or
FRP, (1997). I like to think of it as a neat way of expressing stream
processing networks. Disruptor’s <a
href="https://github.com/LMAX-Exchange/disruptor/wiki/Disruptor-Wizard">“wizard”</a>
DSL and Akka’s <a
href="https://doc.akka.io/docs/akka/current/stream/stream-graphs.html">graph
DSL</a> try to add a high-level syntax for expressing networks, but they
both have a rather imperative rather than declarative feel. It’s however
not clear (to me) how effectively implement, parallelise<a href="#fn2"
class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, or
distribute FRP. Some interesting work has been done with hot code
swapping in the FRP <a
href="https://github.com/turion/essence-of-live-coding">setting</a>,
which is potentially useful for a telling a good upgrade story.</p>
<p>To summarise, while there are many streaming libraries there seem to
be few (at least that I know of) that tick all of the following
boxes:</p>
<ol type="1">
<li>Parallel processing:
<ul>
<li>in a determinate way;</li>
<li>fanning out and sharding without copying data (when run on a single
computer).</li>
</ul></li>
<li>Potentially distributed over multiple computers for fault tolerance
and upgrades, without the need to change the code of the pipeline;</li>
<li>Observable, to ease debugging and performance analysis;</li>
<li>Declarative high-level way of expressing stream processing networks
(i.e. the pipeline);</li>
<li>Good deploy, upgrade, rescale story for stateful systems;</li>
<li>Elastic, i.e. ability to rescale automatically to meet the
load.</li>
</ol>
<p>I think we need all of the above in order to build Barbara’s
“substrate for distributed systems”. We’ll not get all the way there in
this post, but at least this should give you a sense of the direction
I’d like to go.</p>
<h2 id="plan">Plan</h2>
<p>The rest of this post is organised as follows.</p>
<p>First we’ll have a look at how to model pipelines as a transformation
of lists. The purpose of this is to give us an easy to understand
sequential specification of what we would like our pipelines to do.</p>
<p>We’ll then give our first parallel implementation of pipelines using
“normal” queues. The main point here is to recap of the problem with
copying data that arises from using “normal” queues, but we’ll also
sketch how one can test the parallel implementation using the model.</p>
<p>After that we’ll have a look at the Disruptor API, sketch its single
producer implementation and discuss how it helps solve the problems we
identified in the previous section.</p>
<p>Finally we’ll have enough background to be able to sketch the
Disruptor implementation of pipelines. We’ll also discuss how
monitoring/observability can be added.</p>
<h2 id="list-transformer-model">List transformer model</h2>
<p>Let’s first introduce the type for our pipelines. We index our
pipeline datatype by two types, in order to be able to precisely specify
its input and output types. For example, the <code>Id</code>entity
pipeline has the same input as output type, while pipeline composition
(<code>:&gt;&gt;&gt;</code>) expects its first argument to be a pipeline
from <code>a</code> to <code>b</code>, and the second argument a
pipeline from <code>b</code> to <code>c</code> in order for the
resulting composed pipeline to be from <code>a</code> to <code>c</code>
(similar to functional composition).</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">P</span><span class="ot"> ::</span> <span class="dt">Type</span> <span class="ot">-&gt;</span> <span class="dt">Type</span> <span class="ot">-&gt;</span> <span class="dt">Type</span> <span class="kw">where</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">Id</span><span class="ot">      ::</span> <span class="dt">P</span> a a</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ot">  (:&gt;&gt;&gt;)  ::</span> <span class="dt">P</span> a b <span class="ot">-&gt;</span> <span class="dt">P</span> b c <span class="ot">-&gt;</span> <span class="dt">P</span> a c</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">Map</span><span class="ot">     ::</span> (a <span class="ot">-&gt;</span> b) <span class="ot">-&gt;</span> <span class="dt">P</span> a b</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="ot">  (:***)  ::</span> <span class="dt">P</span> a c <span class="ot">-&gt;</span> <span class="dt">P</span> b d <span class="ot">-&gt;</span> <span class="dt">P</span> (a, b) (c, d)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="ot">  (:&amp;&amp;&amp;)  ::</span> <span class="dt">P</span> a b <span class="ot">-&gt;</span> <span class="dt">P</span> a c <span class="ot">-&gt;</span> <span class="dt">P</span> a (b, c)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="ot">  (:+++)  ::</span> <span class="dt">P</span> a c <span class="ot">-&gt;</span> <span class="dt">P</span> b d <span class="ot">-&gt;</span> <span class="dt">P</span> (<span class="dt">Either</span> a b) (<span class="dt">Either</span> c d)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="ot">  (:|||)  ::</span> <span class="dt">P</span> a c <span class="ot">-&gt;</span> <span class="dt">P</span> b c <span class="ot">-&gt;</span> <span class="dt">P</span> (<span class="dt">Either</span> a b) c</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">Shard</span><span class="ot">   ::</span> <span class="dt">P</span> a b <span class="ot">-&gt;</span> <span class="dt">P</span> a b</span></code></pre></div>
<p>Here’s a pipeline that takes a stream of integers as input and
outputs a stream of pairs where the first component is the input integer
and the second component is a boolean indicating if the first component
was an even integer or not.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ot">examplePipeline ::</span> <span class="dt">P</span> <span class="dt">Int</span> (<span class="dt">Int</span>, <span class="dt">Bool</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>examplePipeline <span class="ot">=</span> <span class="dt">Id</span> <span class="op">:&amp;&amp;&amp;</span> <span class="dt">Map</span> <span class="fu">even</span></span></code></pre></div>
<p>So far our pipelines are merely data which describes what we’d like
to do. In order to actually perform a stream transformation we’d need to
give semantics to our pipeline datatype<a href="#fn3"
class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>.</p>
<p>The simplest semantics we can give our pipelines is that in terms of
list transformations.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ot">model ::</span> <span class="dt">P</span> a b <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> [b]</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>model <span class="dt">Id</span>         xs  <span class="ot">=</span> xs</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>model (f <span class="op">:&gt;&gt;&gt;</span> g) xs  <span class="ot">=</span> model g (model f xs)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>model (<span class="dt">Map</span> f)    xs  <span class="ot">=</span> <span class="fu">map</span> f xs</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>model (f <span class="op">:***</span> g) xys <span class="ot">=</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    (xs, ys) <span class="ot">=</span> <span class="fu">unzip</span> xys</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">in</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">zip</span> (model f xs) (model g ys)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>model (f <span class="op">:&amp;&amp;&amp;</span> g) xs <span class="ot">=</span> <span class="fu">zip</span> (model f xs) (model g xs)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>model (f <span class="op">:+++</span> g) es <span class="ot">=</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    (xs, ys) <span class="ot">=</span> partitionEithers es</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  <span class="kw">in</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">-- Note that we pass in the input list, in order to perserve the order.</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    merge es (model f xs) (model g ys)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    merge []             []       []       <span class="ot">=</span> []</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    merge (<span class="dt">Left</span>  _ <span class="op">:</span> es) (l <span class="op">:</span> ls) rs       <span class="ot">=</span> <span class="dt">Left</span>  l <span class="op">:</span> merge es ls rs</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    merge (<span class="dt">Right</span> _ <span class="op">:</span> es) ls       (r <span class="op">:</span> rs) <span class="ot">=</span> <span class="dt">Right</span> r <span class="op">:</span> merge es ls rs</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>model (f <span class="op">:|||</span> g) es <span class="ot">=</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    (xs, ys) <span class="ot">=</span> partitionEithers es</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>  <span class="kw">in</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    merge es (model f xs) (model g ys)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    merge []             []       []       <span class="ot">=</span> []</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    merge (<span class="dt">Left</span>  _ <span class="op">:</span> es) (l <span class="op">:</span> ls) rs       <span class="ot">=</span> l <span class="op">:</span> merge es ls rs</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    merge (<span class="dt">Right</span> _ <span class="op">:</span> es) ls       (r <span class="op">:</span> rs) <span class="ot">=</span> r <span class="op">:</span> merge es ls rs</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>model (<span class="dt">Shard</span> f) xs <span class="ot">=</span> model f xs</span></code></pre></div>
<p>Note that this semantics is completely sequential and preserves the
order of the inputs (determinism). Also note that since we don’t have
parallelism yet, <code>Shard</code>ing doesn’t do anything. We’ll
introduce parallelism without breaking determinism in the next
section.</p>
<p>We can now run our example pipeline in the REPL:</p>
<pre><code>&gt; model examplePipeline [1,2,3,4,5]
[(1,False),(2,True),(3,False),(4,True),(5,False)]</code></pre>
<h2 id="queue-pipeline-deployment">Queue pipeline deployment</h2>
<p>In the previous section we saw how to deploy pipelines in a purely
sequential way in order to process lists. The purpose of this is merely
to give ourselves an intuition of what pipelines should do as well as an
executable model which we can test our intuition against.</p>
<p>Next we shall have a look at our first parallel deployment. The idea
here is to show how we can involve multiple threads in the stream
processing, without making the output non-deterministic (same input
should always give the same output).</p>
<p>We can achieve this as follows:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ot">deploy ::</span> <span class="dt">P</span> a b <span class="ot">-&gt;</span> <span class="dt">TQueue</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">TQueue</span> b)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>deploy <span class="dt">Id</span>         xs <span class="ot">=</span> <span class="fu">return</span> xs</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>deploy (f <span class="op">:&gt;&gt;&gt;</span> g) xs <span class="ot">=</span> deploy g <span class="op">=&lt;&lt;</span> deploy f xs</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>deploy (<span class="dt">Map</span> f)    xs <span class="ot">=</span> deploy (<span class="dt">MapM</span> (<span class="fu">return</span> <span class="op">.</span> f)) xs</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>deploy (<span class="dt">MapM</span> f)   xs <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">-- (Where `MapM :: (a -&gt; IO b) -&gt; P a b` is the monadic generalisation of</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="co">-- `Map` from the list model that we saw earlier.)</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  ys <span class="ot">&lt;-</span> newTQueueIO</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  forkIO <span class="op">$</span> forever <span class="op">$</span> <span class="kw">do</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> atomically (readTQueue xs)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> f x</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    atomically (writeTQueue ys y)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span> ys</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>deploy (f <span class="op">:&amp;&amp;&amp;</span> g) xs <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>  xs1 <span class="ot">&lt;-</span> newTQueueIO</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  xs2 <span class="ot">&lt;-</span> newTQueueIO</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>  forkIO <span class="op">$</span> forever <span class="op">$</span> <span class="kw">do</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> atomically (readTQueue xs)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    atomically <span class="op">$</span> <span class="kw">do</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>      writeTQueue xs1 x</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>      writeTQueue xs2 x</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>  ys <span class="ot">&lt;-</span> deploy f xs1</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>  zs <span class="ot">&lt;-</span> deploy g xs2</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>  yzs <span class="ot">&lt;-</span> newTQueueIO</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>  forkIO <span class="op">$</span> forever <span class="op">$</span> <span class="kw">do</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> atomically (readTQueue ys)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    z <span class="ot">&lt;-</span> atomically (readTQueue zs)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    atomically (writeTQueue yzs (y, z))</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span> yzs</span></code></pre></div>
<p>(I’ve omitted the cases for <code>:|||</code> and <code>:+++</code>
to not take up too much space. We’ll come back and handle
<code>Shard</code> separately later.)</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ot">example&#39; ::</span> [<span class="dt">Int</span>] <span class="ot">-&gt;</span> <span class="dt">IO</span> [(<span class="dt">Int</span>, <span class="dt">Bool</span>)]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>example&#39; xs0 <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  xs <span class="ot">&lt;-</span> newTQueueIO</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mapM_</span> (atomically <span class="op">.</span> writeTQueue xs) xs0</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  ys <span class="ot">&lt;-</span> deploy (<span class="dt">Id</span> <span class="op">:&amp;&amp;&amp;</span> <span class="dt">Map</span> <span class="fu">even</span>) xs</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  replicateM (<span class="fu">length</span> xs0) (atomically (readTQueue ys))</span></code></pre></div>
<p>Running <a
href="https://github.com/stevana/pipelining-with-disruptor/blob/main/src/QueueDeployment.hs">this</a>
in our REPL, gives the same result as in the model:</p>
<pre><code>&gt; example&#39; [1,2,3,4,5]
[(1,False),(2,True),(3,False),(4,True),(5,False)]</code></pre>
<p>In fact, we can use our model to define a property-based test which
asserts that our queue deployment is faithful to the model:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ot">prop_commute ::</span> <span class="dt">Eq</span> b <span class="ot">=&gt;</span> <span class="dt">P</span> a b <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> <span class="dt">PropertyM</span> <span class="dt">IO</span> ()</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>prop_commute p xs <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  ys <span class="ot">&lt;-</span> run <span class="op">$</span> <span class="kw">do</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    qxs <span class="ot">&lt;-</span> newTQueueIO</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mapM_</span> (atomically <span class="op">.</span> writeTQueue qxs) xs</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    qys <span class="ot">&lt;-</span> deploy p qxs</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    replicateM (<span class="fu">length</span> xs) (atomically (readTQueue qys))</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>  assert (model p xs <span class="op">==</span> ys)</span></code></pre></div>
<p>Actually running this property for arbitrary pipelines would require
us to first define a pipeline generator, which is a bit tricky given the
indexes of the datatype<a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>. It can still me used as a helper
for testing specific pipelines though,
e.g. <code>prop_commute examplePipeline</code>.</p>
<p>A bigger problem is that we’ve spawned two threads, when deploying
<code>:&amp;&amp;&amp;</code>, whose mere job is to copy elements from
the input queue (<code>xs</code>) to the input queues of <code>f</code>
and <code>g</code> (<code>xs{1,2}</code>), and from the outputs of
<code>f</code> and <code>g</code> (<code>ys</code> and <code>zs</code>)
to the output of <code>f &amp;&amp;&amp; g</code> (<code>ysz</code>).
Copying data is expensive.</p>
<p>When we shard a pipeline we effectively clone it and send half of the
traffic to one clone and the other half to the other. One way to achieve
this is as follows, notice how in <code>shard</code> we swap
<code>qEven</code> and <code>qOdd</code> when we recurse:</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>deploy (<span class="dt">Shard</span> f) xs <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  xsEven <span class="ot">&lt;-</span> newTQueueIO</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  xsOdd  <span class="ot">&lt;-</span> newTQueueIO</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  _pid   <span class="ot">&lt;-</span> forkIO (shard xs xsEven xsOdd)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  ysEven <span class="ot">&lt;-</span> deploy f xsEven</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  ysOdd  <span class="ot">&lt;-</span> deploy f xsOdd</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  ys     <span class="ot">&lt;-</span> newTQueueIO</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  _pid   <span class="ot">&lt;-</span> forkIO (merge ysEven ysOdd ys)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span> ys</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="ot">    shard ::</span> <span class="dt">TQueue</span> a <span class="ot">-&gt;</span> <span class="dt">TQueue</span> a <span class="ot">-&gt;</span> <span class="dt">TQueue</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    shard  qIn qEven qOdd <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>      atomically (readTQueue qIn <span class="op">&gt;&gt;=</span> writeTQueue qEven)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>      shard qIn qOdd qEven</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="ot">    merge ::</span> <span class="dt">TQueue</span> a <span class="ot">-&gt;</span> <span class="dt">TQueue</span> a <span class="ot">-&gt;</span> <span class="dt">TQueue</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    merge qEven qOdd qOut <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>      atomically (readTQueue qEven <span class="op">&gt;&gt;=</span> writeTQueue qOut)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>      merge qOdd qEven qOut</span></code></pre></div>
<p>This alteration will shard the input queue (<code>qIn</code>) on even
and odd indices, and we can <code>merge</code> it back without losing
determinism. Note that if we’d simply had a pool of worker threads
taking items from the input queue and putting them on the output queue
(<code>qOut</code>) after processing, then we wouldn’t have a
deterministic outcome. Also notice that in the <code>deploy</code>ment
of <code>Shard</code>ing we also end up copying data between the queues,
similar to the fan-out case (<code>:&amp;&amp;&amp;</code>)!</p>
<p>Before we move on to show how to avoid doing this copying, let’s have
a look at a couple of examples to get a better feel for pipelining and
sharding. If we generalise <code>Map</code> to <code>MapM</code> in our
<a
href="https://github.com/stevana/pipelining-with-disruptor/blob/main/src/ModelIO.hs">model</a>
we can write the following contrived program:</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ot">modelSleep ::</span> <span class="dt">P</span> () ()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>modelSleep <span class="ot">=</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">MapM</span> (<span class="fu">const</span> (threadDelay <span class="dv">250000</span>)) <span class="op">:&amp;&amp;&amp;</span> <span class="dt">MapM</span> (<span class="fu">const</span> (threadDelay <span class="dv">250000</span>)) <span class="op">:&gt;&gt;&gt;</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">MapM</span> (<span class="fu">const</span> (threadDelay <span class="dv">250000</span>)) <span class="op">:&gt;&gt;&gt;</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">MapM</span> (<span class="fu">const</span> (threadDelay <span class="dv">250000</span>))</span></code></pre></div>
<p>The argument to <code>threadDelay</code> (or sleep) is microseconds,
so at each point in the pipeline we are sleeping 1/4 of a second.</p>
<p>If we feed this pipeline <code>5</code> items:</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ot">runModelSleep ::</span> <span class="dt">IO</span> ()</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>runModelSleep <span class="ot">=</span> void (model modelSleep (<span class="fu">replicate</span> <span class="dv">5</span> ()))</span></code></pre></div>
<p>We see that it takes roughly 5 seconds:</p>
<pre><code>&gt; :set +s
&gt; runModelSleep
(5.02 secs, 905,480 bytes)</code></pre>
<p>This is expected, even though we pipeline and fan-out, as the model
is completely sequential.</p>
<p>If we instead run the same pipeline using the queue deployment, we
get:</p>
<pre><code>&gt; runQueueSleep
(1.76 secs, 907,160 bytes)</code></pre>
<p>The reason for this is that the two sleeps in the fan-out happen in
parallel now and when the first item is at the second stage the first
stage starts processing the second item, and so on, i.e. we get a
pipelining parallelism.</p>
<p>If we, for some reason, wanted to achieve a sequential running time
using the queue deployment, we’d have to write a one stage pipeline like
so:</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ot">queueSleepSeq ::</span> <span class="dt">P</span> () ()</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>queueSleepSeq <span class="ot">=</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">MapM</span> <span class="op">$</span> \() <span class="ot">-&gt;</span> <span class="kw">do</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    ()       <span class="ot">&lt;-</span> threadDelay <span class="dv">250000</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    ((), ()) <span class="ot">&lt;-</span> (,) <span class="op">&lt;$&gt;</span> threadDelay <span class="dv">250000</span> <span class="op">&lt;*&gt;</span> threadDelay <span class="dv">250000</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    ()       <span class="ot">&lt;-</span> threadDelay <span class="dv">250000</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span> ()</span></code></pre></div>
<pre><code>&gt; runQueueSleepSeq
(5.02 secs, 898,096 bytes)</code></pre>
<p>Using sharding we can get an even shorter running time:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ot">queueSleepSharded ::</span> <span class="dt">P</span> () ()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>queueSleepSharded <span class="ot">=</span> <span class="dt">Shard</span> queueSleep</span></code></pre></div>
<pre><code>&gt; runQueueSleepSharded
(1.26 secs, 920,888 bytes)</code></pre>
<p>This is pretty much where we left off in my previous post. If the
speed ups we are seeing from pipelining don’t make sense, it might help
to go back and reread the <a
href="https://stevana.github.io/pipelined_state_machines.html">old
post</a>, as I spent some more time constructing an intuitive example
there.</p>
<h2 id="disruptor">Disruptor</h2>
<p>Before we can understand how the Disruptor can help us avoid the
problem copying between queues that we just saw, we need to first
understand a bit about how the Disruptor is implemented.</p>
<p>We will be looking at the implementation of the single-producer
Disruptor, because in our pipelines there will never be more than one
producer per queue (the stage before it)<a href="#fn5"
class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a>.</p>
<p>Let’s first have a look at the datatype and then explain each
field:</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">RingBuffer</span> a <span class="ot">=</span> <span class="dt">RingBuffer</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  {<span class="ot"> capacity             ::</span> <span class="dt">Int</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  ,<span class="ot"> elements             ::</span> <span class="dt">IOArray</span> <span class="dt">Int</span> a</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  ,<span class="ot"> cursor               ::</span> <span class="dt">IORef</span> <span class="dt">SequenceNumber</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  ,<span class="ot"> gatingSequences      ::</span> <span class="dt">IORef</span> (<span class="dt">IOArray</span> <span class="dt">Int</span> (<span class="dt">IORef</span> <span class="dt">SequenceNumber</span>))</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  ,<span class="ot"> cachedGatingSequence ::</span> <span class="dt">IORef</span> <span class="dt">SequenceNumber</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="kw">newtype</span> <span class="dt">SequenceNumber</span> <span class="ot">=</span> <span class="dt">SequenceNumber</span> <span class="dt">Int</span></span></code></pre></div>
<p>The Disruptor is a ring buffer queue with a fixed
<code>capacity</code>. It’s backed by an array whose length is equal to
the capacity, this is where the <code>elements</code> of the ring buffer
are stored. There’s a monotonically increasing counter called the
<code>cursor</code> which keeps track of how many elements we have
written. By taking the value of the <code>cursor</code> modulo the
<code>capacity</code> we get the index into the array where we are
supposed to write our next element (this is how we wrap around the
array, i.e. forming a ring). In order to avoid overwriting elements
which have not yet been consumed we also need to keep track of the
cursors of all consumers (<code>gatingSequences</code>). As an
optimisation we cache where the last consumer is
(<code>cachedGatingSequence</code>).</p>
<p>The API from the producing side looks as follows:</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="ot">tryClaimBatch   ::</span> <span class="dt">RingBuffer</span> a <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">Maybe</span> <span class="dt">SequenceNumber</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="ot">writeRingBuffer ::</span> <span class="dt">RingBuffer</span> a <span class="ot">-&gt;</span> <span class="dt">SequenceNumber</span> <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="ot">publish         ::</span> <span class="dt">RingBuffer</span> a <span class="ot">-&gt;</span> <span class="dt">SequenceNumber</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span></code></pre></div>
<p>We first try to claim <code>n :: Int</code> slots in the ring buffer,
if that fails (returns <code>Nothing</code>) then we know that there
isn’t space in the ring buffer and we should apply backpressure upstream
(e.g. if the producer is a web server, we might want to temporarily
rejecting clients with status code 503). Once we successfully get a
sequence number, we can start writing our data. Finally we publish the
sequence number, this makes it available on the consumer side.</p>
<p>The consumer side of the API looks as follows:</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="ot">addGatingSequence ::</span> <span class="dt">RingBuffer</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">IORef</span> <span class="dt">SequenceNumber</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="ot">waitFor           ::</span> <span class="dt">RingBuffer</span> a <span class="ot">-&gt;</span> <span class="dt">SequenceNumber</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">SequenceNumber</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="ot">readRingBuffer    ::</span> <span class="dt">RingBuffer</span> a <span class="ot">-&gt;</span> <span class="dt">SequenceNumber</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> a</span></code></pre></div>
<p>First we need to add a consumer to the ring buffer (to avoid
overwriting on wrap around of the ring), this gives us a consumer
cursor. The consumer is responsible for updating this cursor, the ring
buffer will only read from it to avoid overwriting. After the consumer
reads the cursor, it calls <code>waitFor</code> on the read value, this
will block until an element has been <code>publish</code>ed on that slot
by the producer. In the case that the producer is ahead it will return
the current sequence number of the producer, hence allowing the consumer
to do a batch of reads (from where it currently is to where the producer
currently is). Once the consumer has caught up with the producer it
updates its cursor.</p>
<p>Here’s an example which hopefully makes things more concrete:</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="ot">example ::</span> <span class="dt">IO</span> ()</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>example <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  rb <span class="ot">&lt;-</span> newRingBuffer_ <span class="dv">2</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  c <span class="ot">&lt;-</span> addGatingSequence rb</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> batchSize <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">Just</span> hi <span class="ot">&lt;-</span> tryClaimBatch rb batchSize</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> lo <span class="ot">=</span> hi <span class="op">-</span> (coerce batchSize <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>  assertIO (lo <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>  assertIO (hi <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>  <span class="co">-- Notice that these writes are batched:</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mapM_</span> (\(i, c) <span class="ot">-&gt;</span> writeRingBuffer rb i c) (<span class="fu">zip</span> [lo<span class="op">..</span>hi] [<span class="ch">&#39;a&#39;</span><span class="op">..</span>])</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>  publish rb hi</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>  <span class="co">-- Since the ring buffer size is only two and we&#39;ve written two</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>  <span class="co">-- elements, it&#39;s full at this point:</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>  <span class="dt">Nothing</span> <span class="ot">&lt;-</span> tryClaimBatch rb <span class="dv">1</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>  consumed <span class="ot">&lt;-</span> readIORef c</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>  produced <span class="ot">&lt;-</span> waitFor rb consumed</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>  <span class="co">-- The consumer can do batched reads, and only do some expensive</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>  <span class="co">-- operation once it reaches the end of the batch:</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>  xs <span class="ot">&lt;-</span> <span class="fu">mapM</span> (readRingBuffer rb) [consumed <span class="op">+</span> <span class="dv">1</span><span class="op">..</span>produced]</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>  assertIO (xs <span class="op">==</span> <span class="st">&quot;ab&quot;</span>)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>  <span class="co">-- The consumer updates its cursor:</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>  writeIORef c produced</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>  <span class="co">-- Now there&#39;s space again for the producer:</span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>  <span class="dt">Just</span> <span class="dv">2</span> <span class="ot">&lt;-</span> tryClaimBatch rb <span class="dv">1</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span> ()</span></code></pre></div>
<p>See the <code>Disruptor</code> <a href="src/Disruptor.hs">module</a>
in case you are interested in the implementation details.</p>
<p>Hopefully by now we’ve seen enough internals to be able to explain
why the Disruptor performs well. First of all, by using a ring buffer we
only allocate memory when creating the ring buffer, it’s then reused
when we wrap around the ring. The ring buffer is implemented using an
array, so the memory access patterns are predictable and the CPU can do
prefetching. The consumers don’t have a copy of the data, they merely
have a pointer (the sequence number) to how far in the producer’s ring
buffer they are, which allows for fanning out or sharding to multiple
consumers without copying data. The fact that we can batch on both the
write side (with <code>tryClaimBatch</code>) and on the reader side
(with <code>waitFor</code>) also helps. All this taken together
contributes to the Disruptor’s performance.</p>
<h2 id="disruptor-pipeline-deployment">Disruptor pipeline
deployment</h2>
<p>Recall that the reason we introduced the Disruptor was to avoid
copying elements of the queue when fanning out (using the
<code>:&amp;&amp;&amp;</code> combinator) and sharding.</p>
<p>The idea would be to have the workers we fan-out to both be consumers
of the same Disruptor, that way the inputs don’t need to be copied.
Avoiding to copy the individual outputs from the worker’s queues (of
<code>a</code>s and <code>b</code>s) into the combined output (of
<code>(a, b)</code>s) is a bit trickier.</p>
<p>One way, that I think works, is to do something reminiscent what <a
href="https://hackage.haskell.org/package/vector"><code>Data.Vector</code></a>
does for pairs. That’s a vector of pairs (<code>Vector (a, b)</code>) is
actually represented as a pair of vectors
(<code>(Vector a, Vector b)</code>)<a href="#fn6" class="footnote-ref"
id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
<p>We can achieve this with <a
href="http://simonmar.github.io/bib/papers/assoc.pdf">associated
types</a> as follows:</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> <span class="dt">HasRB</span> a <span class="kw">where</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">data</span> <span class="dt">RB</span><span class="ot"> a ::</span> <span class="dt">Type</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="ot">  newRB               ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">RB</span> a)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="ot">  tryClaimBatchRB     ::</span> <span class="dt">RB</span> a <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">Maybe</span> <span class="dt">SequenceNumber</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="ot">  writeRingBufferRB   ::</span> <span class="dt">RB</span> a <span class="ot">-&gt;</span> <span class="dt">SequenceNumber</span> <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="ot">  publishRB           ::</span> <span class="dt">RB</span> a <span class="ot">-&gt;</span> <span class="dt">SequenceNumber</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="ot">  addGatingSequenceRB ::</span> <span class="dt">RB</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Counter</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="ot">  waitForRB           ::</span> <span class="dt">RB</span> a <span class="ot">-&gt;</span> <span class="dt">SequenceNumber</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">SequenceNumber</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="ot">  readRingBufferRB    ::</span> <span class="dt">RB</span> a <span class="ot">-&gt;</span> <span class="dt">SequenceNumber</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> a</span></code></pre></div>
<p>The instances for this class for types that are not pairs will just
use the Disruptor that we defined above.</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span> <span class="dt">HasRB</span> <span class="dt">String</span> <span class="kw">where</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">data</span> <span class="dt">RB</span> <span class="dt">String</span> <span class="ot">=</span> <span class="dt">RB</span> (<span class="dt">RingBuffer</span> <span class="dt">String</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  newRB n        <span class="ot">=</span> <span class="dt">RB</span> <span class="op">&lt;$&gt;</span> newRingBuffer_ n</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="op">...</span></span></code></pre></div>
<p>While the instance for pairs will use a pair of Disruptors:</p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span> (<span class="dt">HasRB</span> a, <span class="dt">HasRB</span> b) <span class="ot">=&gt;</span> <span class="dt">HasRB</span> (a, b) <span class="kw">where</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">data</span> <span class="dt">RB</span> (a, b) <span class="ot">=</span> <span class="dt">RBPair</span> (<span class="dt">RB</span> a) (<span class="dt">RB</span> b)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>  newRB n <span class="ot">=</span> <span class="dt">RBPair</span> <span class="op">&lt;$&gt;</span> newRB n <span class="op">&lt;*&gt;</span> newRB n</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="op">...</span></span></code></pre></div>
<p>The <code>deploy</code> function for the fan-out combinator can now
avoid copying:</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="ot">deploy ::</span> (<span class="dt">HasRB</span> a, <span class="dt">HasRB</span> b) <span class="ot">=&gt;</span> <span class="dt">P</span> a b <span class="ot">-&gt;</span> <span class="dt">RB</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">RB</span> b)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>deploy (p <span class="op">:&amp;&amp;&amp;</span> q) xs <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>  ys <span class="ot">&lt;-</span> deploy p xs</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>  zs <span class="ot">&lt;-</span> deploy q xs</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span> (<span class="dt">RBPair</span> ys zs)</span></code></pre></div>
<p>Sharding, or partition parallelism as Jim calls it, is a way to make
a copy of a pipeline and divert half of the events to the first copy and
the other half to the other copy. Assuming there are enough unused
CPUs/core, this could effectively double the throughput. It might be
helpful to think of the events at even positions in the stream going to
the first pipeline copy while the events in the odd positions in the
stream go to the second copy of the pipeline.</p>
<p>When we shard in the <code>TQueue</code> deployment of pipelines we
end up copying events from the original stream into the two pipeline
copies. This is similar to copying when fanning out, which we discussed
above, and the solution is similar.</p>
<p>First we need to change the pipeline type so that the shard
constructor has an output type that’s <code>Sharded</code>.</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode diff"><code class="sourceCode diff"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>data P :: Type -&gt; Type -&gt; Type where</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  ...</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="st">- Shard :: P a b -&gt; P a b</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="va">+ Shard :: P a b -&gt; P a (Sharded b)</span></span></code></pre></div>
<p>This type is in fact merely the identity type:</p>
<div class="sourceCode" id="cb28"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">newtype</span> <span class="dt">Sharded</span> a <span class="ot">=</span> <span class="dt">Sharded</span> a</span></code></pre></div>
<p>But it allows us to define a <code>HasRB</code> instance which does
the sharding without copying as follows:</p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span> <span class="dt">HasRB</span> a <span class="ot">=&gt;</span> <span class="dt">HasRB</span> (<span class="dt">Sharded</span> a) <span class="kw">where</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">data</span> <span class="dt">RB</span> (<span class="dt">Sharded</span> a) <span class="ot">=</span> <span class="dt">RBShard</span> <span class="dt">Partition</span> <span class="dt">Partition</span> (<span class="dt">RB</span> a) (<span class="dt">RB</span> a)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>  readRingBufferRB (<span class="dt">RBShard</span> p1 p2 xs ys) i</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">|</span> partition i p1 <span class="ot">=</span> readRingBufferRB xs i</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="op">|</span> partition i p2 <span class="ot">=</span> readRingBufferRB ys i</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">...</span></span></code></pre></div>
<p>The idea being that we split the ring buffer into two, like when
fanning out, and then we have a way of taking an index and figuring out
which of the two ring buffers it’s actually in.</p>
<p>This partitioning information, <code>p</code>, is threaded though
while deploying:</p>
<div class="sourceCode" id="cb30"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>deploy (<span class="dt">Shard</span> f) p xs <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> (p1, p2) <span class="ot">=</span> addPartition p</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>  ys1 <span class="ot">&lt;-</span> deploy f p1 xs</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>  ys2 <span class="ot">&lt;-</span> deploy f p2 xs</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span> (<span class="dt">RBShard</span> p1 p2 ys1 ys2)</span></code></pre></div>
<p>For the details of how this works see the following footnote<a
href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a> and the
<code>HasRB (Sharded a)</code> instance in the following <a
href="https://github.com/stevana/pipelining-with-disruptor/blob/main/src/RingBufferClass.hs">module</a>.</p>
<p>If we <a
href="https://github.com/stevana/pipelining-with-disruptor/blob/main/src/LibMain/Sleep.hs">run</a>
our sleep pipeline from before using the Disruptor <a
href="https://github.com/stevana/pipelining-with-disruptor/blob/main/src/Pipeline.hs">deployment</a>
we get similar timings as with the queue deployment:</p>
<pre><code>&gt; runDisruptorSleep False
(2.01 secs, 383,489,976 bytes)

&gt; runDisruptorSleepSharded False
(1.37 secs, 286,207,264 bytes)</code></pre>
<p>In order to get a better understanding of how not copying when
fanning out and sharding improves performance, let’s instead have a look
at this pipeline which fans out five times:</p>
<div class="sourceCode" id="cb37"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="ot">copyP ::</span> <span class="dt">P</span> () ()</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>copyP <span class="ot">=</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">Id</span> <span class="op">:&amp;&amp;&amp;</span> <span class="dt">Id</span> <span class="op">:&amp;&amp;&amp;</span> <span class="dt">Id</span> <span class="op">:&amp;&amp;&amp;</span> <span class="dt">Id</span> <span class="op">:&amp;&amp;&amp;</span> <span class="dt">Id</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>  <span class="op">:&gt;&gt;&gt;</span> <span class="dt">Map</span> (<span class="fu">const</span> ())</span></code></pre></div>
<p>If we deploy this pipeline using queues and feed it five million
items we get the following statistics from the profiler:</p>
<pre><code>11,457,369,968 bytes allocated in the heap
   198,233,200 bytes copied during GC
     5,210,024 bytes maximum residency (27 sample(s))
     4,841,208 bytes maximum slop
           216 MiB total memory in use (0 MB lost due to fragmentation)


real    0m8.368s
user    0m10.647s
sys     0m0.778s</code></pre>
<p>While the same setup but using the Disruptor deployment gives us:</p>
<pre><code>6,629,305,096 bytes allocated in the heap
  110,544,544 bytes copied during GC
    3,510,424 bytes maximum residency (17 sample(s))
    5,090,472 bytes maximum slop
          214 MiB total memory in use (0 MB lost due to fragmentation)

real    0m5.028s
user    0m7.000s
sys     0m0.626s</code></pre>
<p>So about an half the amount of bytes allocated in the heap using the
Disruptor.</p>
<p>If we double the fan-out factor from five to ten, we get the
following stats with the queue deployment:</p>
<pre><code>35,552,340,768 bytes allocated in the heap
 7,355,365,488 bytes copied during GC
    31,518,256 bytes maximum residency (295 sample(s))
       739,472 bytes maximum slop
           257 MiB total memory in use (0 MB lost due to fragmentation)

real    0m46.104s
user    3m35.192s
sys     0m1.387s</code></pre>
<p>and the following for the Disruptor deployment:</p>
<pre><code>11,457,369,968 bytes allocated in the heap
   198,233,200 bytes copied during GC
     5,210,024 bytes maximum residency (27 sample(s))
     4,841,208 bytes maximum slop
           216 MiB total memory in use (0 MB lost due to fragmentation)

real    0m8.368s
user    0m10.647s
sys     0m0.778s</code></pre>
<p>So it seems that the gap between the two deployments widens as we
introduce more fan-out, this expected as the queue implementation will
have more copying of data to do<a href="#fn8" class="footnote-ref"
id="fnref8" role="doc-noteref"><sup>8</sup></a>.</p>
<h2 id="observability">Observability</h2>
<p>Given that pipelines are directed acyclic graphs and that we have a
concrete datatype constructor for each pipeline combinator, it’s
relatively straight forward to add a visualisation of a deployment.</p>
<p>Furthermore, since each Disruptor has a <code>cursor</code> keeping
that of how many elements it produced and all the consumers of a
Disruptor have one keeping track of how many elements they have
consumed, we can annotate our deployment visualisation with this data
and get a good idea of the progress the pipeline is making over time as
well as spot potential bottlenecks.</p>
<p>Here’s an example of such an visualisation, for a <a
href="https://github.com/stevana/pipelining-with-disruptor/blob/main/src/LibMain/WordCount.hs">word
count</a> pipeline, as an interactive SVG (you need to click on the
image):</p>
<p><a
href="https://stevana.github.io/svg-viewer-in-svg/wordcount-pipeline.svg"><img
src="https://stevana.github.io/svg-viewer-in-svg/wordcount-pipeline.svg"
alt="Demo" /></a></p>
<p>The way it’s implemented is that we spawn a separate thread that read
the producer’s cursors and consumer’s gating sequences
(<code>IORef SequenceNumber</code> in both cases) every millisecond and
saves the <code>SequenceNumber</code>s (integers). After collecting this
data we can create one dot diagram for every time the data changed. In
the demo above, we also collected all the elements of the Disruptor,
this is useful for debugging (the implementation of the pipeline
library), but it would probably be too expensive to enable this when
there’s a lot of items to be processed.</p>
<p>I have written a separate write up on how to make the SVG interactive
over <a
href="https://stevana.github.io/visualising_datastructures_over_time_using_svg.html">here</a>.</p>
<h2 id="running">Running</h2>
<p>All of the above Haskell code is available on <a
href="https://github.com/stevana/pipelining-with-disruptor/">GitHub</a>.
The easiest way to install the right version of GHC and cabal is
probably via <a href="https://www.haskell.org/ghcup/">ghcup</a>. Once
installed the <a
href="https://github.com/stevana/pipelining-with-disruptor/tree/main/src/LibMain">examples</a>
can be run as follows:</p>
<div class="sourceCode" id="cb42"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span> data/test.txt <span class="kw">|</span> <span class="ex">cabal</span> run uppercase</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span> data/test.txt <span class="kw">|</span> <span class="ex">cabal</span> run wc <span class="co"># word count</span></span></code></pre></div>
<p>The <a
href="https://github.com/stevana/pipelining-with-disruptor/blob/main/src/LibMain/Sleep.hs">sleep
examples</a> are run like this:</p>
<div class="sourceCode" id="cb43"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="ex">cabal</span> run sleep</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="ex">cabal</span> run sleep <span class="at">--</span> <span class="at">--sharded</span></span></code></pre></div>
<p>The different <a
href="https://github.com/stevana/pipelining-with-disruptor/blob/main/src/LibMain/Copying.hs">copying
benchmarks</a> can be reproduced as follows:</p>
<div class="sourceCode" id="cb44"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> flag <span class="kw">in</span> <span class="st">&quot;--no-sharding&quot;</span> <span class="dt">\</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;--copy10&quot;</span> <span class="dt">\</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;--tbqueue-no-sharding&quot;</span> <span class="dt">\</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;--tbqueue-copy10&quot;</span><span class="kw">;</span> <span class="cf">do</span> <span class="dt">\</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>  <span class="ex">cabal</span> build copying <span class="kw">&amp;&amp;</span> <span class="dt">\</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">time</span> cabal run copying <span class="at">--</span> <span class="st">&quot;</span><span class="va">$flag</span><span class="st">&quot;</span> <span class="kw">&amp;&amp;</span> <span class="dt">\</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    <span class="ex">eventlog2html</span> copying.eventlog <span class="kw">&amp;&amp;</span> <span class="dt">\</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    <span class="ex">ghc-prof-flamegraph</span> copying.prof <span class="kw">&amp;&amp;</span> <span class="dt">\</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>    <span class="ex">firefox</span> copying.eventlog.html <span class="kw">&amp;&amp;</span> <span class="dt">\</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>    <span class="ex">firefox</span> copying.svg</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="cf">done</span></span></code></pre></div>
<h2 id="further-work-and-contributing">Further work and
contributing</h2>
<p>There’s still a lot to do, but I thought it would be a good place to
stop for now. Here are a bunch of improvements, in no particular
order:</p>
<ul class="task-list">
<li><input type="checkbox" />Implement the <code>Arrow</code> instance
for Disruptor <code>P</code>ipelines, this isn’t as straightforward as
in the model case, because the combinators are littered with
<code>HasRB</code> constraints, e.g.:
<code>(:&amp;&amp;&amp;) :: (HasRB b, HasRB c) =&gt; P a b -&gt;       P a c -&gt; P a (b, c)</code>.
Perhaps taking inspiration from constrained/restricted monads? This
would allow us to specify pipelines using the <a
href="https://ghc.gitlab.haskell.org/ghc/doc/users_guide/exts/arrows.html">arrow
notation</a>.</li>
<li><input type="checkbox" />I believe the current pipeline combinator
allow for arbitrary directed acyclic graphs (DAGs), but what if feedback
cycles are needed? Does an <code>ArrowLoop</code> instance make sense in
that case?</li>
<li><input type="checkbox" />Can we avoid copying when using
<code>Either</code> via <code>(:|||)</code> or <code>(:+++)</code>, e.g.
can we store all <code>Left</code>s in one ring buffer and all
<code>Right</code>s in another?</li>
<li><input type="checkbox" />Use unboxed arrays for types that can be
unboxed in the <code>HasRB</code> instances?</li>
<li><input type="checkbox" />In the word count example we get an input
stream of lines, but we only want to produce a single line as output
when we reach the end of the input stream. In order to do this I added a
way for workers to say that <code>NoOutput</code> was produced in one
step. Currently that constructor still gets written to the output
Disruptor, would it be possible to not write it but still increment the
sequence number counter?</li>
<li><input type="checkbox" />Add more monitoring? Currently we only keep
track of the queue length, i.e. saturation. Adding service time,
i.e. how long it takes to process an item, per worker shouldn’t be hard.
Latency (how long an item has been waiting in the queue) would be more
tricky as we’d need to annotate and propagate a timestamp with the
item?</li>
<li><input type="checkbox" />Since monitoring adds a bit of overheard,
it would be neat to be able to turn monitoring on and off at
runtime;</li>
<li><input type="checkbox" />The <code>HasRB</code> instances are
incomplete, and it’s not clear if they need to be completed? More
testing and examples could help answer this question, or perhaps a
better visualisation?</li>
<li><input type="checkbox" />Actually test using
<code>prop_commute</code> partially applied to a concrete pipeline?</li>
<li><input type="checkbox" />Implement a property-based testing
generator for pipelines and test using <code>prop_commute</code> using
random pipelines?</li>
<li><input type="checkbox" />Add network/HTTP source and sink?</li>
<li><input type="checkbox" />Deploy across network of computers?</li>
<li><input type="checkbox" />Hot-code upgrades of workers/stages with
zero downtime, perhaps continuing on my earlier <a
href="https://stevana.github.io/hot-code_swapping_a_la_erlang_with_arrow-based_state_machines.html">attempt</a>?</li>
<li><input type="checkbox" />In addition to upgrading the
workers/stages, one might also want to rewire the pipeline itself. Doug
made me aware of an old <a
href="https://inria.hal.science/inria-00306565">paper</a> by Gilles Kahn
and David MacQueen (1976), where they reconfigure their network on the
fly. Perhaps some ideas can be stole from there?</li>
<li><input type="checkbox" />Related to reconfiguring is to be able
shard/scale/reroute pipelines and add more machines without downtime.
Can we do this automatically based on our monitoring? Perhaps building
upon my earlier <a
href="https://stevana.github.io/elastically_scalable_thread_pools.html">attempt</a>?</li>
<li><input type="checkbox" />More benchmarks, in particular trying to
confirm that we indeed don’t allocate when fanning out and sharding<a
href="#fn9" class="footnote-ref" id="fnref9"
role="doc-noteref"><sup>9</sup></a>, as well as benchmarks against other
streaming libraries.</li>
</ul>
<p>If any of this seems interesting, feel free to get involved.</p>
<h2 id="see-also">See also</h2>
<ul>
<li>Guy Steele’s talk <a
href="https://www.infoq.com/presentations/Thinking-Parallel-Programming/">How
to Think about Parallel Programming: Not!</a> (2011);</li>
<li><a href="https://youtube.com/watch?v=DCdGlxBbKU4">Understanding the
Disruptor, a Beginner’s Guide to Hardcore Concurrency</a> by Trisha Gee
and Mike Barker (2011);</li>
<li>Mike Barker’s <a
href="https://github.com/mikeb01/folklore/tree/master/src/main/java/performance">brute-force
solution to Guy’s problem and benchmarks</a>;</li>
<li><a
href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/">Streaming
101: The world beyond batch</a> (2015);</li>
<li><a
href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-102/">Streaming
102: The world beyond batch</a> (2016);</li>
<li><a
href="https://people.eecs.berkeley.edu/~brewer/papers/SEDA-sosp.pdf"><em>SEDA:
An Architecture for Well-Conditioned Scalable Internet Services</em></a>
(2001);</li>
<li><a
href="https://www.microsoft.com/en-us/research/publication/naiad-a-timely-dataflow-system-2/">Microsoft
Naiad</a>: a timely dataflow system (with stage notifications)
(2013);</li>
<li>Elixir’s ALF flow-based programming <a
href="https://www.youtube.com/watch?v=2XrYd1W5GLo">library</a>
(2021);</li>
<li><a href="https://mazzo.li/posts/fast-pipes.html">How fast are Linux
pipes anyway?</a> (2022);</li>
<li><a
href="https://man.freebsd.org/cgi/man.cgi?query=netmap&amp;sektion=4">netmap</a>:
a framework for fast packet I/O;</li>
<li><a
href="https://www.gibney.org/the_output_of_linux_pipes_can_be_indeter">The
output of Linux pipes can be indeterministic</a> (2019);</li>
<li><a href="https://www.youtube.com/watch?v=Mc3tTRkjCvE">Programming
Distributed Systems</a> by Mae Milano (Strange Loop, 2023);</li>
<li><a
href="https://www.youtube.com/watch?v=ipceTuJlw-M">Pipeline-oriented
programming</a> by Scott Wlaschin (NDC Porto, 2023).</li>
</ul>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>I noticed that the Wikipedia page for <a
href="https://en.wikipedia.org/wiki/Dataflow_programming">dataflow
programming</a> mentions that Jack Dennis and his graduate students
pioneered that style of programming while he was at MIT in the 60s. I
knew Doug was at MIT around that time as well, and so I sent an email to
Doug asking if he knew of Jack’s work. Doug replied saying he had left
MIT by the 60s, but was still collaborating with people at MIT and was
aware of Jack’s work and also the work by Kelly, Lochbaum and Vyssotsky
on <a href="https://archive.org/details/bstj40-3-669">BLODI</a> (1961)
was on his mind when he wrote the garden hose memo (1964).<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>There’s a paper called <a
href="http://flint.cs.yale.edu/trifonov/papers/pfrp.pdf">Parallel
Functional Reactive Programming</a> by Peterson et al. (2000), but as
Conal Elliott <a
href="http://conal.net/papers/push-pull-frp/push-pull-frp.pdf">points</a>
out:</p>
<blockquote>
<p>“Peterson et al. (2000) explored opportunities for parallelism in
implementing a variation of FRP. While the underlying semantic model was
not spelled out, it seems that semantic determinacy was not preserved,
in contrast to the semantically determinate concurrency used in this
paper (Section 11).”</p>
</blockquote>
<p>Conal’s approach (his Section 11) seems to build upon very fine
grained parallelism provided by an “unambiguous choice” operator which
is implemented by spawning two threads. I don’t understand where exactly
this operator is used in the implementation, but if it’s used every time
an element is processed (in parallel) then the overheard of spawning the
threads could be significant?<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The design space of what pipeline combinators to include
in the pipeline datatype is very big. I’ve chosen the ones I’ve done
because they are instances of already well established type classes:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span> <span class="dt">Category</span> <span class="dt">P</span> <span class="kw">where</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">id</span>    <span class="ot">=</span> <span class="dt">Id</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  g <span class="op">.</span> f <span class="ot">=</span> f <span class="op">:&gt;&gt;&gt;</span> g</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span> <span class="dt">Arrow</span> <span class="dt">P</span> <span class="kw">where</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  arr     <span class="ot">=</span> <span class="dt">Map</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  f <span class="op">***</span> g <span class="ot">=</span> f <span class="op">:***</span> g</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  f <span class="op">&amp;&amp;&amp;</span> g <span class="ot">=</span> f <span class="op">:&amp;&amp;&amp;</span> g</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span> <span class="dt">ArrowChoice</span> <span class="dt">P</span> <span class="kw">where</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  f <span class="op">+++</span> g <span class="ot">=</span> f <span class="op">:+++</span> g</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  f <span class="op">|||</span> g <span class="ot">=</span> f <span class="op">:|||</span> g</span></code></pre></div>
<p>Ideally we’d also like to be able to use <code>Arrow</code>
notation/syntax to describe our pipelines. Even better would be if arrow
notation worked for Cartesian categories. See Conal Elliott’s work on <a
href="http://conal.net/papers/compiling-to-categories/">compiling to
categories</a>, as well as Oleg Grenrus’ GHC <a
href="https://github.com/phadej/overloaded/blob/master/src/Overloaded/Categories.hs">plugin</a>
that does the right thing and translates arrow syntax into Cartesian
categories.<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Search for “QuickCheck GADTs” if you are interested in
finding out more about this topic.<a href="#fnref4"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The Disruptor also comes in a multi-producer variant,
see the following <a
href="https://github.com/stevana/pipelined-state-machines/tree/main/src/Disruptor/MP">repository</a>
for a Haskell version or the <a
href="https://github.com/LMAX-Exchange/disruptor">LMAX</a> repository
for the original Java implementation.<a href="#fnref5"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>See also <a
href="https://en.wikipedia.org/wiki/AoS_and_SoA">array of structures vs
structure of arrays</a> in other programming languages.<a href="#fnref6"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>The partitioning information consists of the total
number of partitions and the index of the current partition.</p>
<div class="sourceCode" id="cb31"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">Partition</span> <span class="ot">=</span> <span class="dt">Partition</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>  {<span class="ot"> pIndex ::</span> <span class="dt">Int</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>  ,<span class="ot"> pTotal ::</span> <span class="dt">Int</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p>No partitioning is represented as follows:</p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="ot">noPartition ::</span> <span class="dt">Partition</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>noPartition <span class="ot">=</span> <span class="dt">Partition</span> <span class="dv">0</span> <span class="dv">1</span></span></code></pre></div>
<p>While creating a new partition is done as follows:</p>
<div class="sourceCode" id="cb33"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="ot">addPartition ::</span> <span class="dt">Partition</span> <span class="ot">-&gt;</span> (<span class="dt">Partition</span>, <span class="dt">Partition</span>)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>addPartition (<span class="dt">Partition</span> i total) <span class="ot">=</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>  ( <span class="dt">Partition</span> i (total <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>  , <span class="dt">Partition</span> (i <span class="op">+</span> total) (total <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>So, for example, if we partition twice we get:</p>
<pre><code>&gt; let (p1, p2) = addPartition noPartition in (addPartition p1, addPartition p2)
((Partition 0 4, Partition 2 4), (Partition 1 4, Partition 3 4))</code></pre>
<p>From this information we can compute if an index is in an partition
or not as follows:</p>
<div class="sourceCode" id="cb35"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="ot">partition ::</span> <span class="dt">SequenceNumber</span> <span class="ot">-&gt;</span> <span class="dt">Partition</span> <span class="ot">-&gt;</span> <span class="dt">Bool</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>partition i (<span class="dt">Partition</span> n total) <span class="ot">=</span> i <span class="ot">`mod`</span> total <span class="op">==</span> <span class="dv">0</span> <span class="op">+</span> n</span></code></pre></div>
<p>To understand why this works, it might be helpful to consider the
case where we only have two partitions. We can partition on even or odd
indices as follows: <code>even i = i `mod` 2 == 0 + 0</code> and
<code>odd i = i `mod` 2 == 0 + 1</code>. Written this way we can easier
see how to generalise to <code>total</code> partitions:
<code>partition i (Partition n total) = i `mod` total == 0 + n</code>.
So for <code>total = 2</code> then
<code>partition i (Partition 0 2) == even</code> while
<code>partition i (Partition 1 2) == odd</code>.</p>
<p>Since partitioning and partitioning a partition, etc, always
introduce a power of two we can further optimise to use bitwise or as
follows:
<code>partition i (Partition n total) = i .|. (total - 1) == 0 + n</code>
thereby avoiding the expensive modulus computation. This is a trick used
in Disruptor as well, and the reason why the capacity of a Disruptor
always needs to be a power of two.<a href="#fnref7"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>I’m not sure why “bytes allocated in the heap” gets
doubled in the Disruptor case and tripled in the queue cases though?<a
href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>The partitioning information consists of the total
number of partitions and the index of the current partition.</p>
<div class="sourceCode" id="cb45"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">Partition</span> <span class="ot">=</span> <span class="dt">Partition</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>  {<span class="ot"> pIndex ::</span> <span class="dt">Int</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>  ,<span class="ot"> pTotal ::</span> <span class="dt">Int</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p>No partitioning is represented as follows:</p>
<div class="sourceCode" id="cb46"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="ot">noPartition ::</span> <span class="dt">Partition</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>noPartition <span class="ot">=</span> <span class="dt">Partition</span> <span class="dv">0</span> <span class="dv">1</span></span></code></pre></div>
<p>While creating a new partition is done as follows:</p>
<div class="sourceCode" id="cb47"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="ot">addPartition ::</span> <span class="dt">Partition</span> <span class="ot">-&gt;</span> (<span class="dt">Partition</span>, <span class="dt">Partition</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>addPartition (<span class="dt">Partition</span> i total) <span class="ot">=</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>  ( <span class="dt">Partition</span> i (total <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>  , <span class="dt">Partition</span> (i <span class="op">+</span> total) (total <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>So, for example, if we partition twice we get:</p>
<pre><code>&gt; let (p1, p2) = addPartition noPartition in (addPartition p1, addPartition p2)
((Partition 0 4, Partition 2 4), (Partition 1 4, Partition 3 4))</code></pre>
<p>From this information we can compute if an index is in an partition
or not as follows:</p>
<div class="sourceCode" id="cb49"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="ot">partition ::</span> <span class="dt">SequenceNumber</span> <span class="ot">-&gt;</span> <span class="dt">Partition</span> <span class="ot">-&gt;</span> <span class="dt">Bool</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>partition i (<span class="dt">Partition</span> n total) <span class="ot">=</span> i <span class="ot">`mod`</span> total <span class="op">==</span> <span class="dv">0</span> <span class="op">+</span> n</span></code></pre></div>
<p>To understand why this works, it might be helpful to consider the
case where we only have two partitions. We can partition on even or odd
indices as follows: <code>even i = i `mod` 2 == 0 + 0</code> and
<code>odd i = i `mod` 2 == 0 + 1</code>. Written this way we can easier
see how to generalise to <code>total</code> partitions:
<code>partition i (Partition n total) = i `mod` total == 0 + n</code>.
So for <code>total = 2</code> then
<code>partition i (Partition 0 2) == even</code> while
<code>partition i (Partition 1 2) == odd</code>.</p>
<p>Since partitioning and partitioning a partition, etc, always
introduce a power of two we can further optimise to use bitwise or as
follows:
<code>partition i (Partition n total) = i .|. (total - 1) == 0 + n</code>
thereby avoiding the expensive modulus computation. This is a trick used
in Disruptor as well, and the reason why the capacity of a Disruptor
always needs to be a power of two.<a href="#fnref9"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</main>
</body>
</html>
